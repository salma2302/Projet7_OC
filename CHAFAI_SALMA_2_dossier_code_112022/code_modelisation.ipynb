{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c601e364",
   "metadata": {},
   "source": [
    "# Projet 7 : Implémentez un modèle de scoring\n",
    "\n",
    "# Notebook de la partie modélisation (du prétraitement à la prédiction)\n",
    "\n",
    "# SALMA CHAFAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e941a519",
   "metadata": {},
   "source": [
    "**Mission:**\n",
    "\n",
    "* Vous êtes Data Scientist au sein d'une société financière, nommée \"Prêt à dépenser\", qui propose des crédits à la consommation pour des personnes ayant peu ou pas du tout d'historique de prêt.\n",
    "    \n",
    "* L’entreprise souhaite mettre en œuvre un outil de “scoring crédit” pour calculer la probabilité qu’un client rembourse son crédit, puis classifie la demande en crédit accordé ou refusé. Elle souhaite donc développer un algorithme de classification en s’appuyant sur des sources de données variées (données comportementales, données provenant d'autres institutions financières, etc.).\n",
    "\n",
    "* De plus, les chargés de relation client ont fait remonter le fait que les clients sont de plus en plus demandeurs de transparence vis-à-vis des décisions d’octroi de crédit. Cette demande de transparence des clients va tout à fait dans le sens des valeurs que l’entreprise veut incarner.\n",
    "\n",
    "* \"Prêt à dépenser\" décide donc de développer un dashboard interactif pour que les chargés de relation client puissent à la fois expliquer de façon la plus transparente possible les décisions d’octroi de crédit, mais également permettre à leurs clients de disposer de leurs informations personnelles et de les explorer facilement. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaabf56",
   "metadata": {},
   "source": [
    "**Résumé de notre mission:**\n",
    "\n",
    " * 1- Construire un modèle de scoring qui donnera une prédiction sur la probabilité de faillite d'un client de façon automatique.\n",
    "\n",
    " * 2- Construire un dashboard interactif à destination des gestionnaires de la relation client permettant d'interpréter les prédictions faites par le modèle, et d’améliorer la connaissance client des chargés de relation client."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57889857",
   "metadata": {},
   "source": [
    "## A) Importation des bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f72e09",
   "metadata": {},
   "source": [
    "### 1- Les bibliothèques usuelles et les bibliothèques de visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74f14b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    " # ça nous permet d'importer numpy avec son nom np et matplotlib.pyplot as plt\n",
    "%pylab inline \n",
    "\n",
    "# data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy \n",
    "\n",
    "# visualisation\n",
    "import seaborn as sns\n",
    "import missingno as msn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Librairie plotly pour les graphiques intéractives\n",
    "import plotly.graph_objects as go\n",
    "import plotly as plo\n",
    "import plotly.express as px\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode,iplot\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dfa640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf775266",
   "metadata": {},
   "source": [
    "### 2- Bibliothèques du ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c1d18fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputation\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "\n",
    "# outliers\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, KBinsDiscretizer, QuantileTransformer\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer, make_column_selector\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# Feature engineering\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, f_classif\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import lime #LIME package\n",
    "import lime.lime_tabular #the type of LIIME analysis we’ll do\n",
    "import shap #SHAP package\n",
    "import time #some of the routines take a while so we monitor the time\n",
    "import os #needed to use Environment Variables in Domino\n",
    "\n",
    "# Equilibrer les classes\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Modèles\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier,BaggingClassifier,GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import kernel_ridge\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "# Evaluation \n",
    "from sklearn.metrics import f1_score, mean_squared_error, mean_absolute_error, confusion_matrix, classification_report, fbeta_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, plot_confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, make_scorer\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold, cross_val_score,StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# pickle est un package qu'on utilise pour mettre dedans ou pour générer notre modèle pour le déployer dans une application\n",
    "import pickle\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c296dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from lightgbm import LGBMClassifier\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c196e53",
   "metadata": {},
   "source": [
    "## B) Jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "038c13cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>...</th>\n",
       "      <th>INSTAL_AMT_INSTALMENT_MEAN</th>\n",
       "      <th>INSTAL_AMT_INSTALMENT_SUM</th>\n",
       "      <th>INSTAL_AMT_PAYMENT_MIN</th>\n",
       "      <th>INSTAL_AMT_PAYMENT_MAX</th>\n",
       "      <th>INSTAL_AMT_PAYMENT_MEAN</th>\n",
       "      <th>INSTAL_AMT_PAYMENT_SUM</th>\n",
       "      <th>INSTAL_DAYS_ENTRY_PAYMENT_MAX</th>\n",
       "      <th>INSTAL_DAYS_ENTRY_PAYMENT_MEAN</th>\n",
       "      <th>INSTAL_DAYS_ENTRY_PAYMENT_SUM</th>\n",
       "      <th>INSTAL_COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100002</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>351000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11559.247</td>\n",
       "      <td>219625.690</td>\n",
       "      <td>9251.775</td>\n",
       "      <td>53093.746</td>\n",
       "      <td>11559.247</td>\n",
       "      <td>219625.690</td>\n",
       "      <td>-49.0</td>\n",
       "      <td>-315.5</td>\n",
       "      <td>-5993.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>1129500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>64754.586</td>\n",
       "      <td>1618864.600</td>\n",
       "      <td>6662.970</td>\n",
       "      <td>560835.400</td>\n",
       "      <td>64754.586</td>\n",
       "      <td>1618864.600</td>\n",
       "      <td>-544.0</td>\n",
       "      <td>-1385.0</td>\n",
       "      <td>-34633.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7096.155</td>\n",
       "      <td>21288.465</td>\n",
       "      <td>5357.250</td>\n",
       "      <td>10573.965</td>\n",
       "      <td>7096.155</td>\n",
       "      <td>21288.465</td>\n",
       "      <td>-727.0</td>\n",
       "      <td>-761.5</td>\n",
       "      <td>-2285.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100006</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>29686.5</td>\n",
       "      <td>297000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>62947.090</td>\n",
       "      <td>1007153.440</td>\n",
       "      <td>2482.920</td>\n",
       "      <td>691786.900</td>\n",
       "      <td>62947.090</td>\n",
       "      <td>1007153.440</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-271.5</td>\n",
       "      <td>-4346.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>21865.5</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12666.444</td>\n",
       "      <td>835985.300</td>\n",
       "      <td>0.180</td>\n",
       "      <td>22678.785</td>\n",
       "      <td>12214.061</td>\n",
       "      <td>806128.000</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>-1032.0</td>\n",
       "      <td>-68128.0</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 624 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  TARGET  CODE_GENDER  FLAG_OWN_CAR  FLAG_OWN_REALTY  \\\n",
       "0      100002       1            0             0                0   \n",
       "1      100003       0            1             0                1   \n",
       "2      100004       0            0             1                0   \n",
       "3      100006       0            1             0                0   \n",
       "4      100007       0            0             0                0   \n",
       "\n",
       "   CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  AMT_GOODS_PRICE  \\\n",
       "0             0          202500.0    406597.5      24700.5         351000.0   \n",
       "1             0          270000.0   1293502.5      35698.5        1129500.0   \n",
       "2             0           67500.0    135000.0       6750.0         135000.0   \n",
       "3             0          135000.0    312682.5      29686.5         297000.0   \n",
       "4             0          121500.0    513000.0      21865.5         513000.0   \n",
       "\n",
       "   ...  INSTAL_AMT_INSTALMENT_MEAN  INSTAL_AMT_INSTALMENT_SUM  \\\n",
       "0  ...                   11559.247                 219625.690   \n",
       "1  ...                   64754.586                1618864.600   \n",
       "2  ...                    7096.155                  21288.465   \n",
       "3  ...                   62947.090                1007153.440   \n",
       "4  ...                   12666.444                 835985.300   \n",
       "\n",
       "   INSTAL_AMT_PAYMENT_MIN  INSTAL_AMT_PAYMENT_MAX  INSTAL_AMT_PAYMENT_MEAN  \\\n",
       "0                9251.775               53093.746                11559.247   \n",
       "1                6662.970              560835.400                64754.586   \n",
       "2                5357.250               10573.965                 7096.155   \n",
       "3                2482.920              691786.900                62947.090   \n",
       "4                   0.180               22678.785                12214.061   \n",
       "\n",
       "   INSTAL_AMT_PAYMENT_SUM  INSTAL_DAYS_ENTRY_PAYMENT_MAX  \\\n",
       "0              219625.690                          -49.0   \n",
       "1             1618864.600                         -544.0   \n",
       "2               21288.465                         -727.0   \n",
       "3             1007153.440                          -12.0   \n",
       "4              806128.000                          -14.0   \n",
       "\n",
       "   INSTAL_DAYS_ENTRY_PAYMENT_MEAN  INSTAL_DAYS_ENTRY_PAYMENT_SUM  INSTAL_COUNT  \n",
       "0                          -315.5                        -5993.0          19.0  \n",
       "1                         -1385.0                       -34633.0          25.0  \n",
       "2                          -761.5                        -2285.0           3.0  \n",
       "3                          -271.5                        -4346.0          16.0  \n",
       "4                         -1032.0                       -68128.0          66.0  \n",
       "\n",
       "[5 rows x 624 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Jeu du données\n",
    "df = pd.read_csv('C:/Users/salma/OneDrive/Bureau/Projet7/Data/data_clean_vf.csv')\n",
    "df.drop(columns = 'Unnamed: 0', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "845bba0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307507, 624)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Taille du jeu de données\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e193681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SK_ID_CURR                             0.0\n",
       "PREV_CODE_REJECT_REASON_VERIF_MEAN     0.0\n",
       "PREV_NAME_PAYMENT_TYPE_nan_MEAN        0.0\n",
       "PREV_CODE_REJECT_REASON_CLIENT_MEAN    0.0\n",
       "PREV_CODE_REJECT_REASON_HC_MEAN        0.0\n",
       "                                      ... \n",
       "ORGANIZATION_TYPE_Trade: type 7        0.0\n",
       "ORGANIZATION_TYPE_Transport: type 1    0.0\n",
       "ORGANIZATION_TYPE_Transport: type 2    0.0\n",
       "ORGANIZATION_TYPE_Transport: type 3    0.0\n",
       "INSTAL_COUNT                           0.0\n",
       "Length: 624, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((df.isna().sum()/df.shape[0])*100).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91924582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fe131f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1463.96 MB\n",
      "Memory usage after optimization is: 412.91 MB\n",
      "Decreased by 71.8%\n"
     ]
    }
   ],
   "source": [
    "df_reduce = reduce_mem_usage(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0c5dda",
   "metadata": {},
   "source": [
    "### 1- On prend un échantillon du jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c41a5e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>...</th>\n",
       "      <th>INSTAL_AMT_INSTALMENT_MEAN</th>\n",
       "      <th>INSTAL_AMT_INSTALMENT_SUM</th>\n",
       "      <th>INSTAL_AMT_PAYMENT_MIN</th>\n",
       "      <th>INSTAL_AMT_PAYMENT_MAX</th>\n",
       "      <th>INSTAL_AMT_PAYMENT_MEAN</th>\n",
       "      <th>INSTAL_AMT_PAYMENT_SUM</th>\n",
       "      <th>INSTAL_DAYS_ENTRY_PAYMENT_MAX</th>\n",
       "      <th>INSTAL_DAYS_ENTRY_PAYMENT_MEAN</th>\n",
       "      <th>INSTAL_DAYS_ENTRY_PAYMENT_SUM</th>\n",
       "      <th>INSTAL_COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70537</th>\n",
       "      <td>181833</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>360000.0</td>\n",
       "      <td>1928304.0</td>\n",
       "      <td>79578.0</td>\n",
       "      <td>1800000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>49032.132812</td>\n",
       "      <td>3.530314e+06</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>902486.812500</td>\n",
       "      <td>45371.347656</td>\n",
       "      <td>3.266737e+06</td>\n",
       "      <td>-190.0</td>\n",
       "      <td>-1114.0</td>\n",
       "      <td>-80190.0</td>\n",
       "      <td>72.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203889</th>\n",
       "      <td>336380</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>354276.0</td>\n",
       "      <td>19912.5</td>\n",
       "      <td>292500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6731.884277</td>\n",
       "      <td>8.886087e+05</td>\n",
       "      <td>2.070000</td>\n",
       "      <td>58712.941406</td>\n",
       "      <td>6728.795898</td>\n",
       "      <td>8.882011e+05</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>-1490.0</td>\n",
       "      <td>-196680.0</td>\n",
       "      <td>132.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3347</th>\n",
       "      <td>103912</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>315000.0</td>\n",
       "      <td>545040.0</td>\n",
       "      <td>35617.5</td>\n",
       "      <td>450000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>22335.443359</td>\n",
       "      <td>4.690443e+05</td>\n",
       "      <td>538.380005</td>\n",
       "      <td>22365.404297</td>\n",
       "      <td>14895.289062</td>\n",
       "      <td>3.128011e+05</td>\n",
       "      <td>-976.0</td>\n",
       "      <td>-1164.0</td>\n",
       "      <td>-24448.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128439</th>\n",
       "      <td>248989</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>306000.0</td>\n",
       "      <td>1305000.0</td>\n",
       "      <td>38155.5</td>\n",
       "      <td>1305000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>20891.949219</td>\n",
       "      <td>1.483328e+06</td>\n",
       "      <td>37.395000</td>\n",
       "      <td>414450.000000</td>\n",
       "      <td>18686.765625</td>\n",
       "      <td>1.326760e+06</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-933.5</td>\n",
       "      <td>-66271.0</td>\n",
       "      <td>71.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16326</th>\n",
       "      <td>119040</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>148500.0</td>\n",
       "      <td>312768.0</td>\n",
       "      <td>22374.0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>5263.145996</td>\n",
       "      <td>1.315787e+05</td>\n",
       "      <td>164.294998</td>\n",
       "      <td>5591.790039</td>\n",
       "      <td>5064.461914</td>\n",
       "      <td>1.266115e+05</td>\n",
       "      <td>-1687.0</td>\n",
       "      <td>-2078.0</td>\n",
       "      <td>-51942.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 624 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        SK_ID_CURR  TARGET  CODE_GENDER  FLAG_OWN_CAR  FLAG_OWN_REALTY  \\\n",
       "70537       181833       0            1             0                0   \n",
       "203889      336380       0            1             0                0   \n",
       "3347        103912       0            0             0                0   \n",
       "128439      248989       0            0             1                0   \n",
       "16326       119040       0            0             0                0   \n",
       "\n",
       "        CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
       "70537              0          360000.0   1928304.0      79578.0   \n",
       "203889             0          135000.0    354276.0      19912.5   \n",
       "3347               1          315000.0    545040.0      35617.5   \n",
       "128439             3          306000.0   1305000.0      38155.5   \n",
       "16326              2          148500.0    312768.0      22374.0   \n",
       "\n",
       "        AMT_GOODS_PRICE  ...  INSTAL_AMT_INSTALMENT_MEAN  \\\n",
       "70537         1800000.0  ...                49032.132812   \n",
       "203889         292500.0  ...                 6731.884277   \n",
       "3347           450000.0  ...                22335.443359   \n",
       "128439        1305000.0  ...                20891.949219   \n",
       "16326          270000.0  ...                 5263.145996   \n",
       "\n",
       "        INSTAL_AMT_INSTALMENT_SUM  INSTAL_AMT_PAYMENT_MIN  \\\n",
       "70537                3.530314e+06               27.000000   \n",
       "203889               8.886087e+05                2.070000   \n",
       "3347                 4.690443e+05              538.380005   \n",
       "128439               1.483328e+06               37.395000   \n",
       "16326                1.315787e+05              164.294998   \n",
       "\n",
       "        INSTAL_AMT_PAYMENT_MAX  INSTAL_AMT_PAYMENT_MEAN  \\\n",
       "70537            902486.812500             45371.347656   \n",
       "203889            58712.941406              6728.795898   \n",
       "3347              22365.404297             14895.289062   \n",
       "128439           414450.000000             18686.765625   \n",
       "16326              5591.790039              5064.461914   \n",
       "\n",
       "        INSTAL_AMT_PAYMENT_SUM  INSTAL_DAYS_ENTRY_PAYMENT_MAX  \\\n",
       "70537             3.266737e+06                         -190.0   \n",
       "203889            8.882011e+05                          -16.0   \n",
       "3347              3.128011e+05                         -976.0   \n",
       "128439            1.326760e+06                          -10.0   \n",
       "16326             1.266115e+05                        -1687.0   \n",
       "\n",
       "        INSTAL_DAYS_ENTRY_PAYMENT_MEAN  INSTAL_DAYS_ENTRY_PAYMENT_SUM  \\\n",
       "70537                          -1114.0                       -80190.0   \n",
       "203889                         -1490.0                      -196680.0   \n",
       "3347                           -1164.0                       -24448.0   \n",
       "128439                          -933.5                       -66271.0   \n",
       "16326                          -2078.0                       -51942.0   \n",
       "\n",
       "        INSTAL_COUNT  \n",
       "70537           72.0  \n",
       "203889         132.0  \n",
       "3347            21.0  \n",
       "128439          71.0  \n",
       "16326           25.0  \n",
       "\n",
       "[5 rows x 624 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On prend un échantillon de 200 clients pour la partie test du dashboard\n",
    "df_sub = df_reduce.sample(200)\n",
    "#df_sub.drop(columns='TARGET', inplace=True)\n",
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4475a6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.to_csv('echantillon.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "440f5b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique(df_sub[\"TARGET\"].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cd0115",
   "metadata": {},
   "source": [
    "## C) Modélisation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64331694",
   "metadata": {},
   "source": [
    "### 1- Séparation du data set en train et test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a846518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "deleted_cols = ['TARGET','SK_ID_CURR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f08bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reduce.drop(columns=deleted_cols)\n",
    "y = df_reduce['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "babf1b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, \n",
    "                                                    test_size=0.2,\n",
    "                                                   random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2df1e83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    226201\n",
       "1     19804\n",
       "Name: TARGET, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cdc4325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307507, 622)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b513c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    56481\n",
       "1     5021\n",
       "Name: TARGET, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1acfba7",
   "metadata": {},
   "source": [
    "### 2- Sélection des features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a1e0e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le jeu de données \"HomeCredit_columns_description\"\n",
    "col_description = pd.read_csv(\"C:/Users/salma/OneDrive/Bureau/Projet7/Data/Data_kaggle/HomeCredit_columns_description_encode.csv\", sep=',', encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0bb5892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features [254 259 275 284 363 371 374 400 405 410 420 428 433 448 462 468 472 481\n",
      " 493 499 517 594] are constant.\n",
      "invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "# Create and fit selector\n",
    "selector = SelectKBest(f_classif, k=20)\n",
    "selector.fit(X, y)\n",
    "# Get columns to keep and create new dataframe with those only\n",
    "cols = selector.get_support(indices=True)\n",
    "X_new = X.iloc[:,cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb6e6720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>REGION_RATING_CLIENT</th>\n",
       "      <th>REGION_RATING_CLIENT_W_CITY</th>\n",
       "      <th>EXT_SOURCE_1</th>\n",
       "      <th>EXT_SOURCE_2</th>\n",
       "      <th>EXT_SOURCE_3</th>\n",
       "      <th>DAYS_EMPLOYED_PERC</th>\n",
       "      <th>NAME_INCOME_TYPE_Working</th>\n",
       "      <th>NAME_EDUCATION_TYPE_Higher education</th>\n",
       "      <th>BURO_DAYS_CREDIT_MIN</th>\n",
       "      <th>BURO_DAYS_CREDIT_MEAN</th>\n",
       "      <th>BURO_DAYS_CREDIT_UPDATE_MEAN</th>\n",
       "      <th>BURO_CREDIT_ACTIVE_Active_MEAN</th>\n",
       "      <th>BURO_CREDIT_ACTIVE_Closed_MEAN</th>\n",
       "      <th>PREV_NAME_CONTRACT_STATUS_Approved_MEAN</th>\n",
       "      <th>PREV_NAME_CONTRACT_STATUS_Refused_MEAN</th>\n",
       "      <th>PREV_CODE_REJECT_REASON_SCOFR_MEAN</th>\n",
       "      <th>PREV_CODE_REJECT_REASON_XAP_MEAN</th>\n",
       "      <th>PREV_NAME_PRODUCT_TYPE_walk-in_MEAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-9461</td>\n",
       "      <td>-637.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.083008</td>\n",
       "      <td>0.262939</td>\n",
       "      <td>0.139404</td>\n",
       "      <td>0.067322</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1437.0</td>\n",
       "      <td>-874.0</td>\n",
       "      <td>-500.00</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.750</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-16765</td>\n",
       "      <td>-1188.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.311279</td>\n",
       "      <td>0.622070</td>\n",
       "      <td>0.535156</td>\n",
       "      <td>0.070862</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2586.0</td>\n",
       "      <td>-1401.0</td>\n",
       "      <td>-816.00</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.750</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-19046</td>\n",
       "      <td>-225.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.505859</td>\n",
       "      <td>0.556152</td>\n",
       "      <td>0.729492</td>\n",
       "      <td>0.011810</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1326.0</td>\n",
       "      <td>-867.0</td>\n",
       "      <td>-532.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-19005</td>\n",
       "      <td>-3040.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.505859</td>\n",
       "      <td>0.650391</td>\n",
       "      <td>0.535156</td>\n",
       "      <td>0.159912</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1827.0</td>\n",
       "      <td>-1051.0</td>\n",
       "      <td>-481.75</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.555664</td>\n",
       "      <td>0.111084</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.888672</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-19932</td>\n",
       "      <td>-3038.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.505859</td>\n",
       "      <td>0.322754</td>\n",
       "      <td>0.535156</td>\n",
       "      <td>0.152466</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1149.0</td>\n",
       "      <td>-1149.0</td>\n",
       "      <td>-783.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DAYS_BIRTH  DAYS_EMPLOYED  REGION_RATING_CLIENT  \\\n",
       "0       -9461         -637.0                     2   \n",
       "1      -16765        -1188.0                     1   \n",
       "2      -19046         -225.0                     2   \n",
       "3      -19005        -3040.0                     2   \n",
       "4      -19932        -3038.0                     2   \n",
       "\n",
       "   REGION_RATING_CLIENT_W_CITY  EXT_SOURCE_1  EXT_SOURCE_2  EXT_SOURCE_3  \\\n",
       "0                            2      0.083008      0.262939      0.139404   \n",
       "1                            1      0.311279      0.622070      0.535156   \n",
       "2                            2      0.505859      0.556152      0.729492   \n",
       "3                            2      0.505859      0.650391      0.535156   \n",
       "4                            2      0.505859      0.322754      0.535156   \n",
       "\n",
       "   DAYS_EMPLOYED_PERC  NAME_INCOME_TYPE_Working  \\\n",
       "0            0.067322                       1.0   \n",
       "1            0.070862                       0.0   \n",
       "2            0.011810                       1.0   \n",
       "3            0.159912                       1.0   \n",
       "4            0.152466                       1.0   \n",
       "\n",
       "   NAME_EDUCATION_TYPE_Higher education  BURO_DAYS_CREDIT_MIN  \\\n",
       "0                                   0.0               -1437.0   \n",
       "1                                   1.0               -2586.0   \n",
       "2                                   0.0               -1326.0   \n",
       "3                                   0.0               -1827.0   \n",
       "4                                   0.0               -1149.0   \n",
       "\n",
       "   BURO_DAYS_CREDIT_MEAN  BURO_DAYS_CREDIT_UPDATE_MEAN  \\\n",
       "0                 -874.0                       -500.00   \n",
       "1                -1401.0                       -816.00   \n",
       "2                 -867.0                       -532.00   \n",
       "3                -1051.0                       -481.75   \n",
       "4                -1149.0                       -783.00   \n",
       "\n",
       "   BURO_CREDIT_ACTIVE_Active_MEAN  BURO_CREDIT_ACTIVE_Closed_MEAN  \\\n",
       "0                           0.250                           0.750   \n",
       "1                           0.250                           0.750   \n",
       "2                           0.000                           1.000   \n",
       "3                           0.375                           0.625   \n",
       "4                           0.000                           1.000   \n",
       "\n",
       "   PREV_NAME_CONTRACT_STATUS_Approved_MEAN  \\\n",
       "0                                 1.000000   \n",
       "1                                 1.000000   \n",
       "2                                 1.000000   \n",
       "3                                 0.555664   \n",
       "4                                 1.000000   \n",
       "\n",
       "   PREV_NAME_CONTRACT_STATUS_Refused_MEAN  PREV_CODE_REJECT_REASON_SCOFR_MEAN  \\\n",
       "0                                0.000000                                 0.0   \n",
       "1                                0.000000                                 0.0   \n",
       "2                                0.000000                                 0.0   \n",
       "3                                0.111084                                 0.0   \n",
       "4                                0.000000                                 0.0   \n",
       "\n",
       "   PREV_CODE_REJECT_REASON_XAP_MEAN  PREV_NAME_PRODUCT_TYPE_walk-in_MEAN  \n",
       "0                          1.000000                             0.000000  \n",
       "1                          1.000000                             0.000000  \n",
       "2                          1.000000                             0.000000  \n",
       "3                          0.888672                             0.000000  \n",
       "4                          1.000000                             0.166626  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f241230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train = X_new.columns.tolist()\n",
    "len(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13610503",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_description = []\n",
    "cols_desc = col_description['Row'].tolist()\n",
    "for feature in features_train : \n",
    "    if feature in cols_desc :\n",
    "        ligne = col_description[col_description['Row'] == feature]['Description'].iloc[0]\n",
    "        l_description.append(ligne)\n",
    "    else : \n",
    "        l_description.append('Pas de description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be96a320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrer les colonnes et leurs descriptions\n",
    "pickle.dump(features_train, open('features_selected.pkl', 'wb'))\n",
    "pickle.dump(l_description, open('features_description.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced4d0f5",
   "metadata": {},
   "source": [
    "### 3- Modélisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fefe86d",
   "metadata": {},
   "source": [
    "#### (a) Division du dataset en train et test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ba2d004",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_new,y, \n",
    "                                                    test_size=0.2,\n",
    "                                                   random_state=42,\n",
    "                                                   stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f8ff85",
   "metadata": {},
   "source": [
    "#### (b) Création de la fonction coût métier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe86d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fct_cout(y_test, y_pred, cout_fp=1, coup_fn=10):\n",
    "    \n",
    "    TN, FP, FN, TP = confusion_matrix(y_test, y_pred).ravel()\n",
    "    resultat = (cout_fp*FP + coup_fn*FN)/(TN + coup_fn*FN + TP + cout_fp*FP)\n",
    "    return resultat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3025b9",
   "metadata": {},
   "source": [
    "On transforme la fct_cout en un score en utilisant la fonction make_scorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b898aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_cout = make_scorer(fct_cout, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ae2ace",
   "metadata": {},
   "source": [
    "#### (c) Fonctions d'entraînement des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "065f88ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrainement_model(model, params,X_train,X_test,y_train, y_test):\n",
    "    \n",
    "    \"\"\"Cette fonction renvoie le meilleur modèle et les meilleurs hyperparamètres\n",
    "    obtenues par cross validation avec GridSearchCv\"\"\"\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    \n",
    "    grid_model = GridSearchCV(model, params, cv=5, scoring=score_cout)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    grid_model.fit(X_train,y_train)\n",
    "    tf = time.time()\n",
    "    print(\"Temps d'exécution avec gridSearchCv : {:.4f} seconds\".format(tf - t0))\n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    best_model = grid_model.best_estimator_\n",
    "    best_params = grid_model.best_params_\n",
    "    \n",
    "    for mean, std, params in zip(grid_model.cv_results_['mean_test_score'],\n",
    "                                grid_model.cv_results_['std_test_score'],\n",
    "                                grid_model.cv_results_['params']):\n",
    "        \n",
    "        print(\"\\tscore coût = %0.3f (+/-%0.3f) for %s\" % (mean,std*2,params))\n",
    "    \n",
    "\n",
    "    \n",
    "    return best_model, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2e93bb",
   "metadata": {},
   "source": [
    "#### (d) Fonctions d'évaluation des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "520a1b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_model(model, X_train,X_test,y_train, y_test, smote_method = False):\n",
    "    \n",
    "    \"\"\"Cette fonction entraîne le modèle et affiche les scores du modèle\"\"\"\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    if smote_method:\n",
    "        oversampler=SMOTE(random_state=0)\n",
    "        X_train,y_train=oversampler.fit_resample(X_train,y_train)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    tf = time.time()\n",
    "    tps_ex = tf - t0\n",
    "    print(\"Temps d'exécution du meilleur modèle : {:.4f} seconds\".format(tps_ex))\n",
    "    \n",
    "    \n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    acc_train = accuracy_score(y_train, y_pred_train)\n",
    "    acc_test = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    roc_train = roc_auc_score(y_train, y_pred_train)\n",
    "    roc_test = roc_auc_score(y_test, y_pred_test)\n",
    "    \n",
    "    beta_score_train = fbeta_score(y_train, y_pred_train, beta=10)\n",
    "    beta_score_test = fbeta_score(y_test, y_pred_test, beta=10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Accuracy train : \", acc_train)\n",
    "    print(\"Accuracy test : \", acc_test)\n",
    "    print(\"Roc Accuracy train : \", roc_train)\n",
    "    print(\"Roc Accuracy test : \", roc_test)\n",
    "    print(\"Beta score train : \", beta_score_train)\n",
    "    print(\"Beta score test : \", beta_score_test)\n",
    "    print(confusion_matrix(y_test, y_pred_test))\n",
    "    print(classification_report(y_test, y_pred_test))\n",
    "    \n",
    "    return acc_train, acc_test, roc_train, roc_test, beta_score_train, beta_score_test, tps_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "818c7c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les modèles\n",
    "model_lr = LogisticRegression(random_state=0, class_weight=\"balanced\")\n",
    "model_knn = KNeighborsClassifier()\n",
    "model_rf = RandomForestClassifier(random_state=0, class_weight=\"balanced\")\n",
    "model_lgb = LGBMClassifier(random_state=0, class_weight=\"balanced\")\n",
    "model_xgb = XGBClassifier(random_state=0, class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43085fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lr = {'C': [0.1, 1, 10, 100]}\n",
    "\n",
    "#params_knn = {'n_neighbors' : [3,5,7]}\n",
    " \n",
    "\n",
    "params_rf = { \n",
    "    'n_estimators': [100, 200],#[int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n",
    "    'max_depth' : [2, 5, 10], #[int(x) for x in np.linspace(10, 110, num = 11)],\n",
    "    'min_samples_split' : [2, 5, 10]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "params_lgb = {'learning_rate': [0.1, 0.01, 0.001],\n",
    "              'max_depth': [3, 4, 5]}\n",
    "\n",
    "params_xgb = {'learning_rate': [0.1, 0.01, 0.001],\n",
    "              'max_depth': [3, 4, 5]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e746dce",
   "metadata": {},
   "source": [
    "#### (a) Pour la régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c1013fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution avec gridSearchCv : 46.4549 seconds\n",
      "\tscore coût = -0.453 (+/-0.005) for {'C': 0.1}\n",
      "\tscore coût = -0.453 (+/-0.005) for {'C': 1}\n",
      "\tscore coût = -0.453 (+/-0.005) for {'C': 10}\n",
      "\tscore coût = -0.453 (+/-0.005) for {'C': 100}\n",
      "Temps d'exécution du meilleur modèle : 2.5627 seconds\n",
      "Accuracy train :  0.6828316497632162\n",
      "Accuracy test :  0.6849370752170661\n",
      "Roc Accuracy train :  0.6715778312834204\n",
      "Roc Accuracy test :  0.6769715564356691\n",
      "Beta score train :  0.6376691959211847\n",
      "Beta score test :  0.646740348572091\n",
      "[[38811 17726]\n",
      " [ 1651  3314]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.69      0.80     56537\n",
      "           1       0.16      0.67      0.25      4965\n",
      "\n",
      "    accuracy                           0.68     61502\n",
      "   macro avg       0.56      0.68      0.53     61502\n",
      "weighted avg       0.89      0.68      0.76     61502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_lr, best_param_lr = entrainement_model(model_lr, params_lr,X_train,X_test,y_train, y_test)\n",
    "final_model_lr = LogisticRegression(**best_param_lr, random_state=0, class_weight=\"balanced\")\n",
    "acc_train_lr, acc_test_lr, roc_train_lr, roc_test_lr, beta_score_train_lr, beta_score_test_lr, tps_ex_lr = \\\n",
    "evaluation_model(final_model_lr,X_train,X_test,y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf35054d",
   "metadata": {},
   "source": [
    "#### (b) Pour la forêt aléatoire "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d70a076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution avec gridSearchCv : 4121.8280 seconds\n",
      "\tscore coût = -0.480 (+/-0.009) for {'max_depth': 2, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "\tscore coût = -0.477 (+/-0.006) for {'max_depth': 2, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "\tscore coût = -0.480 (+/-0.009) for {'max_depth': 2, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "\tscore coût = -0.477 (+/-0.006) for {'max_depth': 2, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "\tscore coût = -0.480 (+/-0.009) for {'max_depth': 2, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "\tscore coût = -0.477 (+/-0.006) for {'max_depth': 2, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "\tscore coût = -0.457 (+/-0.005) for {'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "\tscore coût = -0.457 (+/-0.005) for {'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "\tscore coût = -0.457 (+/-0.005) for {'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "\tscore coût = -0.457 (+/-0.005) for {'max_depth': 5, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "\tscore coût = -0.457 (+/-0.005) for {'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "\tscore coût = -0.457 (+/-0.005) for {'max_depth': 5, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "\tscore coût = -0.436 (+/-0.008) for {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "\tscore coût = -0.435 (+/-0.008) for {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "\tscore coût = -0.436 (+/-0.007) for {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "\tscore coût = -0.435 (+/-0.007) for {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "\tscore coût = -0.435 (+/-0.007) for {'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "\tscore coût = -0.435 (+/-0.007) for {'max_depth': 10, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "Temps d'exécution du meilleur modèle : 96.8803 seconds\n",
      "Accuracy train :  0.7357695981788988\n",
      "Accuracy test :  0.7256674579688466\n",
      "Roc Accuracy train :  0.717181876865757\n",
      "Roc Accuracy test :  0.6727610349760527\n",
      "Beta score train :  0.6771605849768888\n",
      "Beta score test :  0.594266620793413\n",
      "[[41603 14934]\n",
      " [ 1938  3027]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.74      0.83     56537\n",
      "           1       0.17      0.61      0.26      4965\n",
      "\n",
      "    accuracy                           0.73     61502\n",
      "   macro avg       0.56      0.67      0.55     61502\n",
      "weighted avg       0.89      0.73      0.79     61502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_rf, best_param_rf = entrainement_model(model_rf, params_rf,X_train,X_test,y_train, y_test)\n",
    "final_model_rf = RandomForestClassifier(**best_param_rf, random_state=0, class_weight=\"balanced\")\n",
    "acc_train_rf, acc_test_rf, roc_train_rf, roc_test_rf, beta_score_train_rf, beta_score_test_rf, tps_ex_rf = \\\n",
    "evaluation_model(final_model_rf, X_train,X_test,y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07da91d",
   "metadata": {},
   "source": [
    "#### (c) Le modèle lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5fabad32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution avec gridSearchCv : 138.4898 seconds\n",
      "\tscore coût = -0.450 (+/-0.004) for {'learning_rate': 0.1, 'max_depth': 3}\n",
      "\tscore coût = -0.447 (+/-0.003) for {'learning_rate': 0.1, 'max_depth': 4}\n",
      "\tscore coût = -0.446 (+/-0.005) for {'learning_rate': 0.1, 'max_depth': 5}\n",
      "\tscore coût = -0.471 (+/-0.006) for {'learning_rate': 0.01, 'max_depth': 3}\n",
      "\tscore coût = -0.467 (+/-0.006) for {'learning_rate': 0.01, 'max_depth': 4}\n",
      "\tscore coût = -0.463 (+/-0.007) for {'learning_rate': 0.01, 'max_depth': 5}\n",
      "\tscore coût = -0.480 (+/-0.023) for {'learning_rate': 0.001, 'max_depth': 3}\n",
      "\tscore coût = -0.477 (+/-0.018) for {'learning_rate': 0.001, 'max_depth': 4}\n",
      "\tscore coût = -0.467 (+/-0.013) for {'learning_rate': 0.001, 'max_depth': 5}\n",
      "Temps d'exécution du meilleur modèle : 3.6142 seconds\n",
      "Accuracy train :  0.6920875591959513\n",
      "Accuracy test :  0.6888719066046632\n",
      "Roc Accuracy train :  0.6968446084051136\n",
      "Roc Accuracy test :  0.6823268887127023\n",
      "Beta score train :  0.6808181691686896\n",
      "Beta score test :  0.6537880193595275\n",
      "[[39018 17519]\n",
      " [ 1616  3349]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.69      0.80     56537\n",
      "           1       0.16      0.67      0.26      4965\n",
      "\n",
      "    accuracy                           0.69     61502\n",
      "   macro avg       0.56      0.68      0.53     61502\n",
      "weighted avg       0.90      0.69      0.76     61502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_lgb, best_param_lgb = entrainement_model(model_lgb, params_lgb,X_train,X_test,y_train, y_test)\n",
    "final_model_lgb = LGBMClassifier(**best_param_lgb, random_state=0, class_weight=\"balanced\")\n",
    "acc_train_lgb, acc_test_lgb, roc_train_lgb, roc_test_lgb, beta_score_train_lgb, beta_score_test_lgb, tps_ex_lgb = \\\n",
    "evaluation_model(final_model_lgb, X_train,X_test,y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b837c2",
   "metadata": {},
   "source": [
    "#### (d) Le modèle XgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9d3ca0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:27:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:27:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:27:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:27:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:28:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:28:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:28:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:28:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:28:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:28:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:28:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:28:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:28:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:28:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:28:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:28:58] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:29:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:29:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:29:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:29:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:29:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:29:37] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:29:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:29:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:30:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:30:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:30:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:30:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:30:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:30:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:31:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:31:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:31:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:31:14] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:31:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:31:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:31:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:31:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:31:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:31:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:31:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:31:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:32:06] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:32:06] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:32:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:32:26] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:32:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:32:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:32:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:32:51] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:33:04] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:33:04] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:33:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:33:25] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:33:42] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:33:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:33:59] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:34:00] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:34:16] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:34:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:34:34] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:34:34] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:34:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:34:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:34:53] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:34:53] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:35:02] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:35:02] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:35:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:35:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:35:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:35:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:35:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:35:39] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:35:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:35:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:36:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:36:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:36:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:36:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:36:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:36:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:36:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:36:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:37:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:37:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:37:29] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:37:29] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:37:46] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:37:46] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:38:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:38:04] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Temps d'exécution avec gridSearchCv : 633.4388 seconds\n",
      "\tscore coût = -0.466 (+/-0.001) for {'learning_rate': 0.1, 'max_depth': 3}\n",
      "\tscore coût = -0.465 (+/-0.000) for {'learning_rate': 0.1, 'max_depth': 4}\n",
      "\tscore coût = -0.465 (+/-0.000) for {'learning_rate': 0.1, 'max_depth': 5}\n",
      "\tscore coût = -0.468 (+/-0.000) for {'learning_rate': 0.01, 'max_depth': 3}\n",
      "\tscore coût = -0.468 (+/-0.000) for {'learning_rate': 0.01, 'max_depth': 4}\n",
      "\tscore coût = -0.467 (+/-0.000) for {'learning_rate': 0.01, 'max_depth': 5}\n",
      "\tscore coût = -0.468 (+/-0.000) for {'learning_rate': 0.001, 'max_depth': 3}\n",
      "\tscore coût = -0.467 (+/-0.000) for {'learning_rate': 0.001, 'max_depth': 4}\n",
      "\tscore coût = -0.467 (+/-0.000) for {'learning_rate': 0.001, 'max_depth': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:38:40] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[18:38:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Temps d'exécution du meilleur modèle : 67.9952 seconds\n",
      "Accuracy train :  0.8239602909637622\n",
      "Accuracy test :  0.7945757861532958\n",
      "Roc Accuracy train :  0.8239602909637622\n",
      "Roc Accuracy test :  0.6299542541284401\n",
      "Beta score train :  0.8215438633730442\n",
      "Beta score test :  0.4276572102856582\n",
      "[[46715  9822]\n",
      " [ 2812  2153]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.83      0.88     56537\n",
      "           1       0.18      0.43      0.25      4965\n",
      "\n",
      "    accuracy                           0.79     61502\n",
      "   macro avg       0.56      0.63      0.57     61502\n",
      "weighted avg       0.88      0.79      0.83     61502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_xgb, best_param_xgb = entrainement_model(model_xgb, params_xgb,X_train,X_test,y_train, y_test)\n",
    "final_model_xgb = XGBClassifier(**best_param_xgb, random_state=0, class_weight=\"balanced\")\n",
    "acc_train_xgb, acc_test_xgb, roc_train_xgb, roc_test_xgb, beta_score_train_xgb, beta_score_test_xgb, tps_ex_xgb = \\\n",
    "evaluation_model(final_model_xgb, X_train,X_test,y_train, y_test, smote_method=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b0dcb5",
   "metadata": {},
   "source": [
    "### 4- Tableau comparatif des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3b683588",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comparaison = pd.DataFrame(columns = ['Modèle', 'best_paramètres', 'Accuracy train', 'Accuracy test', \n",
    "                                         'ROC train','ROC test', 'Beta_score train',\n",
    "                                         'Beta_score test',\"temps_exec\"])\n",
    "\n",
    "name_models = [\"Régression logistique\",\"Forêt aléatoire\",\"LightGBM\",\"XGBoost\"]\n",
    "params_models = [best_param_lr, best_param_rf, best_param_lgb,  best_param_xgb]\n",
    "l_acc_train = [acc_train_lr, acc_train_rf,acc_train_lgb,acc_train_xgb]\n",
    "l_acc_test = [acc_test_lr, acc_test_rf,acc_test_lgb, acc_test_xgb]\n",
    "l_roc_train = [roc_train_lr, roc_train_rf,roc_train_lgb, roc_train_xgb]\n",
    "l_roc_test = [roc_test_lr, roc_test_rf,roc_test_lgb,roc_test_xgb]\n",
    "l_betasc_train = [beta_score_train_lr, beta_score_train_rf,beta_score_train_lgb,beta_score_train_xgb]\n",
    "l_betasc_test = [beta_score_test_lr, beta_score_test_rf,beta_score_test_lgb,beta_score_test_xgb]\n",
    "l_tps_ex = [tps_ex_lr, tps_ex_rf, tps_ex_lgb, tps_ex_xgb]\n",
    "\n",
    "# Résultast\n",
    "df_comparaison['Modèle'] = name_models\n",
    "df_comparaison['best_paramètres'] = params_models\n",
    "df_comparaison['Accuracy train'] = l_acc_train\n",
    "df_comparaison['Accuracy test'] = l_acc_test\n",
    "df_comparaison['ROC train'] = l_roc_train\n",
    "df_comparaison['ROC test'] = l_roc_test\n",
    "df_comparaison['Beta_score train'] = l_betasc_train\n",
    "df_comparaison['Beta_score test'] = l_betasc_test\n",
    "df_comparaison['temps_exec'] = l_tps_ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4dd3a9a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modèle</th>\n",
       "      <th>best_paramètres</th>\n",
       "      <th>Accuracy train</th>\n",
       "      <th>Accuracy test</th>\n",
       "      <th>ROC train</th>\n",
       "      <th>ROC test</th>\n",
       "      <th>Beta_score train</th>\n",
       "      <th>Beta_score test</th>\n",
       "      <th>temps_exec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Régression logistique</td>\n",
       "      <td>{'C': 100}</td>\n",
       "      <td>0.682832</td>\n",
       "      <td>0.684937</td>\n",
       "      <td>0.671578</td>\n",
       "      <td>0.676972</td>\n",
       "      <td>0.637669</td>\n",
       "      <td>0.646740</td>\n",
       "      <td>1.833568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Forêt aléatoire</td>\n",
       "      <td>{'max_depth': 10, 'min_samples_split': 2, 'n_e...</td>\n",
       "      <td>0.735770</td>\n",
       "      <td>0.725667</td>\n",
       "      <td>0.717182</td>\n",
       "      <td>0.672761</td>\n",
       "      <td>0.677161</td>\n",
       "      <td>0.594267</td>\n",
       "      <td>96.880283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 5}</td>\n",
       "      <td>0.692088</td>\n",
       "      <td>0.688872</td>\n",
       "      <td>0.696845</td>\n",
       "      <td>0.682327</td>\n",
       "      <td>0.680818</td>\n",
       "      <td>0.653788</td>\n",
       "      <td>3.614241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 5}</td>\n",
       "      <td>0.920201</td>\n",
       "      <td>0.919547</td>\n",
       "      <td>0.508958</td>\n",
       "      <td>0.506581</td>\n",
       "      <td>0.018710</td>\n",
       "      <td>0.014236</td>\n",
       "      <td>22.633902</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Modèle                                    best_paramètres  \\\n",
       "0  Régression logistique                                         {'C': 100}   \n",
       "1        Forêt aléatoire  {'max_depth': 10, 'min_samples_split': 2, 'n_e...   \n",
       "2               LightGBM             {'learning_rate': 0.1, 'max_depth': 5}   \n",
       "3                XGBoost             {'learning_rate': 0.1, 'max_depth': 5}   \n",
       "\n",
       "   Accuracy train  Accuracy test  ROC train  ROC test  Beta_score train  \\\n",
       "0        0.682832       0.684937   0.671578  0.676972          0.637669   \n",
       "1        0.735770       0.725667   0.717182  0.672761          0.677161   \n",
       "2        0.692088       0.688872   0.696845  0.682327          0.680818   \n",
       "3        0.920201       0.919547   0.508958  0.506581          0.018710   \n",
       "\n",
       "   Beta_score test  temps_exec  \n",
       "0         0.646740    1.833568  \n",
       "1         0.594267   96.880283  \n",
       "2         0.653788    3.614241  \n",
       "3         0.014236   22.633902  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comparaison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44ea56b",
   "metadata": {},
   "source": [
    "### 5- Modèle final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c42ed83",
   "metadata": {},
   "source": [
    "On choisit comme modèle final le model LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a79a9cf",
   "metadata": {},
   "source": [
    "#### (a) Définition du seuil de proba pour la fonction coût personnalisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "3a239c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_seuil(model, X_test, y_test) :\n",
    "    \n",
    "    y_proba = model.predict_proba(X_test)[:,1]\n",
    "    best_seuil = 0\n",
    "    best_cost = float('inf')\n",
    "    \n",
    "    list_seuils = np.linspace(start=0, stop=1, num=100)\n",
    "\n",
    "    for seuil in list_seuils:\n",
    "        y_pred = (y_proba >= seuil).astype(int)\n",
    "        #print(\"y pred : \", y_pred)\n",
    "        total_cost = f1_score(y_test, y_pred)\n",
    "        if total_cost < best_cost:\n",
    "            best_seuil = seuil \n",
    "            best_cost = total_cost\n",
    "            \n",
    "        \n",
    "            \n",
    "    y_pred_f = (y_proba >= best_seuil).astype(int)\n",
    "    print(\"prediction : \", y_pred_f)\n",
    "            \n",
    "    return best_seuil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d3bd9097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_model_final(model, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    \"\"\"Cette fonction donne les scores pour le model final choisi\"\"\"\n",
    "    \n",
    "    \n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    acc_train = accuracy_score(y_train, y_pred_train)\n",
    "    acc_test = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    roc_train = roc_auc_score(y_train, y_pred_train)\n",
    "    roc_test = roc_auc_score(y_test, y_pred_test)\n",
    "    \n",
    "    \n",
    "    beta_score_train = fbeta_score(y_train, y_pred_train, beta=10)\n",
    "    beta_score_test = fbeta_score(y_test, y_pred_test, beta=10)\n",
    "    \n",
    "    #sc_cout = score_cout(model, y_test, y_pred_test)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Accuracy train : \", acc_train)\n",
    "    print(\"Accuracy test : \", acc_test)\n",
    "    print(\"Roc Accuracy train : \", roc_train)\n",
    "    print(\"Roc Accuracy test : \", roc_test)\n",
    "    print(\"Beta score train : \", beta_score_train)\n",
    "    print(\"Beta score test : \", beta_score_test)\n",
    "    #print(\"Score de la fonction personnalisée :\", sc_cout)\n",
    "    print(confusion_matrix(y_test, y_pred_test))\n",
    "    print(classification_report(y_test, y_pred_test))    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9235dc",
   "metadata": {},
   "source": [
    "#### (b) On met le modèle final dans un Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f2255928",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_f = best_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "aebb2b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "smt = SMOTE(random_state=42)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c7481a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "                     ('scaler', scaler),\n",
    "                    ('model', model_f)\n",
    "                          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "15191e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('model',\n",
       "                 LGBMClassifier(class_weight='balanced', max_depth=5,\n",
       "                                random_state=0))])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7a05c3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction :  [0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9595959595959597"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = pred_seuil(pipeline, X_test, y_test)\n",
    "res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "11c5fe7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44376605, 0.38042285, 0.58473601, ..., 0.21261666, 0.52236856,\n",
       "       0.5989343 ])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24549eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_model_final(pipeline,X_train, y_train, X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a457de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrer le model logistic regression\n",
    "#pickle.dump(pipeline_lgb, open('model_credit_rl.pkl', 'wb'))\n",
    "pickle.dump(pipeline, open('model_credit.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242ccacd",
   "metadata": {},
   "source": [
    "### 5- Interprétabilité en utilisant shap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66709116",
   "metadata": {},
   "source": [
    "#### (a) Interprétabilité globale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f63888",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pickle.load(open(\"model_credit.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06789f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit la liste des colonnes\n",
    "all_features = list(X_new.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac72639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un explicateur SHAP\n",
    "explainer = shap.TreeExplainer(pipeline.named_steps['model'], X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afd0799",
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump(explainer, open('shap_explainer.dill', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a313945",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = dill.load(open(\"shap_explainer.dill\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a718a2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les valeurs SHAP pour les prédictions de test\n",
    "shap_values = explainer.shap_values(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58bd16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test, feature_names=all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5411aebd",
   "metadata": {},
   "source": [
    "#### (b) Interprétabilité locale Pour un client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ec3996",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sample = df[df[\"SK_ID_CURR\"] == 137021][all_features]\n",
    "X_test_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc64bb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test_sample[all_features].columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266e49dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = pipeline.predict_proba(X_test_sample)\n",
    "pred_proba[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1032a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.predict(X_test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e40660",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer.shap_values(X_test_sample[all_features])\n",
    "shap_values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdb4c7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "shap.plots._waterfall.waterfall_legacy(explainer.expected_value,shap_values[0],feature_names = X_test_sample.columns) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0880c09f",
   "metadata": {},
   "source": [
    "## D) MlFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e916b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04503663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.models.signature import infer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa11378",
   "metadata": {},
   "outputs": [],
   "source": [
    "signature = infer_signature(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1679bea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.sklearn.save_model(pipeline, 'mlflow_model', signature=signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4718f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13e7db2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
