{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c601e364",
   "metadata": {},
   "source": [
    "# Projet 7 : Implémentez un modèle de scoring\n",
    "\n",
    "# Notebook de la partie modélisation (du prétraitement à la prédiction)\n",
    "\n",
    "# SALMA CHAFAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e941a519",
   "metadata": {},
   "source": [
    "**Mission:**\n",
    "\n",
    "* Vous êtes Data Scientist au sein d'une société financière, nommée \"Prêt à dépenser\", qui propose des crédits à la consommation pour des personnes ayant peu ou pas du tout d'historique de prêt.\n",
    "    \n",
    "* L’entreprise souhaite mettre en œuvre un outil de “scoring crédit” pour calculer la probabilité qu’un client rembourse son crédit, puis classifie la demande en crédit accordé ou refusé. Elle souhaite donc développer un algorithme de classification en s’appuyant sur des sources de données variées (données comportementales, données provenant d'autres institutions financières, etc.).\n",
    "\n",
    "* De plus, les chargés de relation client ont fait remonter le fait que les clients sont de plus en plus demandeurs de transparence vis-à-vis des décisions d’octroi de crédit. Cette demande de transparence des clients va tout à fait dans le sens des valeurs que l’entreprise veut incarner.\n",
    "\n",
    "* \"Prêt à dépenser\" décide donc de développer un dashboard interactif pour que les chargés de relation client puissent à la fois expliquer de façon la plus transparente possible les décisions d’octroi de crédit, mais également permettre à leurs clients de disposer de leurs informations personnelles et de les explorer facilement. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaabf56",
   "metadata": {},
   "source": [
    "**Résumé de notre mission:**\n",
    "\n",
    " * 1- Construire un modèle de scoring qui donnera une prédiction sur la probabilité de faillite d'un client de façon automatique.\n",
    "\n",
    " * 2- Construire un dashboard interactif à destination des gestionnaires de la relation client permettant d'interpréter les prédictions faites par le modèle, et d’améliorer la connaissance client des chargés de relation client."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57889857",
   "metadata": {},
   "source": [
    "## A) Importation des bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f72e09",
   "metadata": {},
   "source": [
    "### 1- Les bibliothèques usuelles et les bibliothèques de visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74f14b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    " # ça nous permet d'importer numpy avec son nom np et matplotlib.pyplot as plt\n",
    "%pylab inline \n",
    "\n",
    "# data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy \n",
    "\n",
    "# visualisation\n",
    "import seaborn as sns\n",
    "import missingno as msn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Librairie plotly pour les graphiques intéractives\n",
    "import plotly.graph_objects as go\n",
    "import plotly as plo\n",
    "import plotly.express as px\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode,iplot\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "66162d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import altair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "56c9e0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version de altair==4.2.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Version de altair=={altair.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4dfa640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf775266",
   "metadata": {},
   "source": [
    "### 2- Bibliothèques du ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c1d18fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n"
     ]
    }
   ],
   "source": [
    "# Imputation\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "\n",
    "# outliers\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, KBinsDiscretizer, QuantileTransformer\n",
    "from sklearn.compose import make_column_transformer, ColumnTransformer, make_column_selector\n",
    "from category_encoders import TargetEncoder\n",
    "\n",
    "# Feature engineering\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, f_classif\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import lime #LIME package\n",
    "import lime.lime_tabular #the type of LIIME analysis we’ll do\n",
    "import shap #SHAP package\n",
    "import time #some of the routines take a while so we monitor the time\n",
    "import os #needed to use Environment Variables in Domino\n",
    "\n",
    "# Equilibrer les classes\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Modèles\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier,BaggingClassifier,GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import kernel_ridge\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "\n",
    "# Evaluation \n",
    "from sklearn.metrics import f1_score, mean_squared_error, mean_absolute_error, confusion_matrix, classification_report, fbeta_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, plot_confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, make_scorer\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold, cross_val_score,StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# pickle est un package qu'on utilise pour mettre dedans ou pour générer notre modèle pour le déployer dans une application\n",
    "import pickle\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c296dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from lightgbm import LGBMClassifier\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c196e53",
   "metadata": {},
   "source": [
    "## B) Jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "038c13cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>...</th>\n",
       "      <th>INSTAL_AMT_INSTALMENT_MEAN</th>\n",
       "      <th>INSTAL_AMT_INSTALMENT_SUM</th>\n",
       "      <th>INSTAL_AMT_PAYMENT_MIN</th>\n",
       "      <th>INSTAL_AMT_PAYMENT_MAX</th>\n",
       "      <th>INSTAL_AMT_PAYMENT_MEAN</th>\n",
       "      <th>INSTAL_AMT_PAYMENT_SUM</th>\n",
       "      <th>INSTAL_DAYS_ENTRY_PAYMENT_MAX</th>\n",
       "      <th>INSTAL_DAYS_ENTRY_PAYMENT_MEAN</th>\n",
       "      <th>INSTAL_DAYS_ENTRY_PAYMENT_SUM</th>\n",
       "      <th>INSTAL_COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100002</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>202500.0</td>\n",
       "      <td>406597.5</td>\n",
       "      <td>24700.5</td>\n",
       "      <td>351000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>11559.247</td>\n",
       "      <td>219625.690</td>\n",
       "      <td>9251.775</td>\n",
       "      <td>53093.746</td>\n",
       "      <td>11559.247</td>\n",
       "      <td>219625.690</td>\n",
       "      <td>-49.0</td>\n",
       "      <td>-315.5</td>\n",
       "      <td>-5993.0</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100003</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>270000.0</td>\n",
       "      <td>1293502.5</td>\n",
       "      <td>35698.5</td>\n",
       "      <td>1129500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>64754.586</td>\n",
       "      <td>1618864.600</td>\n",
       "      <td>6662.970</td>\n",
       "      <td>560835.400</td>\n",
       "      <td>64754.586</td>\n",
       "      <td>1618864.600</td>\n",
       "      <td>-544.0</td>\n",
       "      <td>-1385.0</td>\n",
       "      <td>-34633.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67500.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>6750.0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7096.155</td>\n",
       "      <td>21288.465</td>\n",
       "      <td>5357.250</td>\n",
       "      <td>10573.965</td>\n",
       "      <td>7096.155</td>\n",
       "      <td>21288.465</td>\n",
       "      <td>-727.0</td>\n",
       "      <td>-761.5</td>\n",
       "      <td>-2285.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100006</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>312682.5</td>\n",
       "      <td>29686.5</td>\n",
       "      <td>297000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>62947.090</td>\n",
       "      <td>1007153.440</td>\n",
       "      <td>2482.920</td>\n",
       "      <td>691786.900</td>\n",
       "      <td>62947.090</td>\n",
       "      <td>1007153.440</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-271.5</td>\n",
       "      <td>-4346.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100007</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>121500.0</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>21865.5</td>\n",
       "      <td>513000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12666.444</td>\n",
       "      <td>835985.300</td>\n",
       "      <td>0.180</td>\n",
       "      <td>22678.785</td>\n",
       "      <td>12214.061</td>\n",
       "      <td>806128.000</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>-1032.0</td>\n",
       "      <td>-68128.0</td>\n",
       "      <td>66.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 624 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   SK_ID_CURR  TARGET  CODE_GENDER  FLAG_OWN_CAR  FLAG_OWN_REALTY  \\\n",
       "0      100002       1            0             0                0   \n",
       "1      100003       0            1             0                1   \n",
       "2      100004       0            0             1                0   \n",
       "3      100006       0            1             0                0   \n",
       "4      100007       0            0             0                0   \n",
       "\n",
       "   CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  AMT_GOODS_PRICE  \\\n",
       "0             0          202500.0    406597.5      24700.5         351000.0   \n",
       "1             0          270000.0   1293502.5      35698.5        1129500.0   \n",
       "2             0           67500.0    135000.0       6750.0         135000.0   \n",
       "3             0          135000.0    312682.5      29686.5         297000.0   \n",
       "4             0          121500.0    513000.0      21865.5         513000.0   \n",
       "\n",
       "   ...  INSTAL_AMT_INSTALMENT_MEAN  INSTAL_AMT_INSTALMENT_SUM  \\\n",
       "0  ...                   11559.247                 219625.690   \n",
       "1  ...                   64754.586                1618864.600   \n",
       "2  ...                    7096.155                  21288.465   \n",
       "3  ...                   62947.090                1007153.440   \n",
       "4  ...                   12666.444                 835985.300   \n",
       "\n",
       "   INSTAL_AMT_PAYMENT_MIN  INSTAL_AMT_PAYMENT_MAX  INSTAL_AMT_PAYMENT_MEAN  \\\n",
       "0                9251.775               53093.746                11559.247   \n",
       "1                6662.970              560835.400                64754.586   \n",
       "2                5357.250               10573.965                 7096.155   \n",
       "3                2482.920              691786.900                62947.090   \n",
       "4                   0.180               22678.785                12214.061   \n",
       "\n",
       "   INSTAL_AMT_PAYMENT_SUM  INSTAL_DAYS_ENTRY_PAYMENT_MAX  \\\n",
       "0              219625.690                          -49.0   \n",
       "1             1618864.600                         -544.0   \n",
       "2               21288.465                         -727.0   \n",
       "3             1007153.440                          -12.0   \n",
       "4              806128.000                          -14.0   \n",
       "\n",
       "   INSTAL_DAYS_ENTRY_PAYMENT_MEAN  INSTAL_DAYS_ENTRY_PAYMENT_SUM  INSTAL_COUNT  \n",
       "0                          -315.5                        -5993.0          19.0  \n",
       "1                         -1385.0                       -34633.0          25.0  \n",
       "2                          -761.5                        -2285.0           3.0  \n",
       "3                          -271.5                        -4346.0          16.0  \n",
       "4                         -1032.0                       -68128.0          66.0  \n",
       "\n",
       "[5 rows x 624 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Jeu du données\n",
    "df = pd.read_csv('C:/Users/salma/OneDrive/Bureau/Projet7/Data/data_clean_vf.csv')\n",
    "df.drop(columns = 'Unnamed: 0', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "845bba0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307507, 624)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Taille du jeu de données\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e193681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SK_ID_CURR                             0.0\n",
       "PREV_CODE_REJECT_REASON_VERIF_MEAN     0.0\n",
       "PREV_NAME_PAYMENT_TYPE_nan_MEAN        0.0\n",
       "PREV_CODE_REJECT_REASON_CLIENT_MEAN    0.0\n",
       "PREV_CODE_REJECT_REASON_HC_MEAN        0.0\n",
       "                                      ... \n",
       "ORGANIZATION_TYPE_Trade: type 7        0.0\n",
       "ORGANIZATION_TYPE_Transport: type 1    0.0\n",
       "ORGANIZATION_TYPE_Transport: type 2    0.0\n",
       "ORGANIZATION_TYPE_Transport: type 3    0.0\n",
       "INSTAL_COUNT                           0.0\n",
       "Length: 624, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((df.isna().sum()/df.shape[0])*100).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91924582",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fe131f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 1463.96 MB\n",
      "Memory usage after optimization is: 412.91 MB\n",
      "Decreased by 71.8%\n"
     ]
    }
   ],
   "source": [
    "df_reduce = reduce_mem_usage(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0c5dda",
   "metadata": {},
   "source": [
    "### 1- On prend un échantillon du jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c41a5e72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>CODE_GENDER</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>FLAG_OWN_REALTY</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_INCOME_TOTAL</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>...</th>\n",
       "      <th>INSTAL_AMT_INSTALMENT_MEAN</th>\n",
       "      <th>INSTAL_AMT_INSTALMENT_SUM</th>\n",
       "      <th>INSTAL_AMT_PAYMENT_MIN</th>\n",
       "      <th>INSTAL_AMT_PAYMENT_MAX</th>\n",
       "      <th>INSTAL_AMT_PAYMENT_MEAN</th>\n",
       "      <th>INSTAL_AMT_PAYMENT_SUM</th>\n",
       "      <th>INSTAL_DAYS_ENTRY_PAYMENT_MAX</th>\n",
       "      <th>INSTAL_DAYS_ENTRY_PAYMENT_MEAN</th>\n",
       "      <th>INSTAL_DAYS_ENTRY_PAYMENT_SUM</th>\n",
       "      <th>INSTAL_COUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68219</th>\n",
       "      <td>179108</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>76500.0</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7874.509277</td>\n",
       "      <td>1.732392e+05</td>\n",
       "      <td>144.899994</td>\n",
       "      <td>8855.099609</td>\n",
       "      <td>7472.004395</td>\n",
       "      <td>1.643841e+05</td>\n",
       "      <td>-85.0</td>\n",
       "      <td>-634.000</td>\n",
       "      <td>-13944.0</td>\n",
       "      <td>22.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69323</th>\n",
       "      <td>180403</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>157500.0</td>\n",
       "      <td>373140.0</td>\n",
       "      <td>37035.0</td>\n",
       "      <td>337500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>9144.922852</td>\n",
       "      <td>1.097391e+05</td>\n",
       "      <td>4005.135010</td>\n",
       "      <td>24729.929688</td>\n",
       "      <td>9144.922852</td>\n",
       "      <td>1.097391e+05</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>-615.000</td>\n",
       "      <td>-7379.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147830</th>\n",
       "      <td>271407</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>612000.0</td>\n",
       "      <td>848745.0</td>\n",
       "      <td>46174.5</td>\n",
       "      <td>675000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7979.403809</td>\n",
       "      <td>1.994851e+05</td>\n",
       "      <td>4.005000</td>\n",
       "      <td>20831.939453</td>\n",
       "      <td>7979.403809</td>\n",
       "      <td>1.994851e+05</td>\n",
       "      <td>-31.0</td>\n",
       "      <td>-134.750</td>\n",
       "      <td>-3368.0</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11904</th>\n",
       "      <td>113877</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55746.0</td>\n",
       "      <td>781920.0</td>\n",
       "      <td>25101.0</td>\n",
       "      <td>675000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8504.150391</td>\n",
       "      <td>1.870913e+06</td>\n",
       "      <td>7.560000</td>\n",
       "      <td>20717.865234</td>\n",
       "      <td>7712.538574</td>\n",
       "      <td>1.696758e+06</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>-1252.000</td>\n",
       "      <td>-275528.0</td>\n",
       "      <td>220.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244391</th>\n",
       "      <td>382890</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>1525405.5</td>\n",
       "      <td>44599.5</td>\n",
       "      <td>1332000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6453.739258</td>\n",
       "      <td>1.355285e+05</td>\n",
       "      <td>4.545000</td>\n",
       "      <td>42710.398438</td>\n",
       "      <td>6453.739258</td>\n",
       "      <td>1.355285e+05</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>-143.625</td>\n",
       "      <td>-3017.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 624 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        SK_ID_CURR  TARGET  CODE_GENDER  FLAG_OWN_CAR  FLAG_OWN_REALTY  \\\n",
       "68219       179108       0            1             0                0   \n",
       "69323       180403       0            0             1                0   \n",
       "147830      271407       0            1             1                0   \n",
       "11904       113877       0            1             0                0   \n",
       "244391      382890       0            0             1                0   \n",
       "\n",
       "        CNT_CHILDREN  AMT_INCOME_TOTAL  AMT_CREDIT  AMT_ANNUITY  \\\n",
       "68219              0           76500.0    180000.0       9000.0   \n",
       "69323              0          157500.0    373140.0      37035.0   \n",
       "147830             0          612000.0    848745.0      46174.5   \n",
       "11904              0           55746.0    781920.0      25101.0   \n",
       "244391             2          225000.0   1525405.5      44599.5   \n",
       "\n",
       "        AMT_GOODS_PRICE  ...  INSTAL_AMT_INSTALMENT_MEAN  \\\n",
       "68219          180000.0  ...                 7874.509277   \n",
       "69323          337500.0  ...                 9144.922852   \n",
       "147830         675000.0  ...                 7979.403809   \n",
       "11904          675000.0  ...                 8504.150391   \n",
       "244391        1332000.0  ...                 6453.739258   \n",
       "\n",
       "        INSTAL_AMT_INSTALMENT_SUM  INSTAL_AMT_PAYMENT_MIN  \\\n",
       "68219                1.732392e+05              144.899994   \n",
       "69323                1.097391e+05             4005.135010   \n",
       "147830               1.994851e+05                4.005000   \n",
       "11904                1.870913e+06                7.560000   \n",
       "244391               1.355285e+05                4.545000   \n",
       "\n",
       "        INSTAL_AMT_PAYMENT_MAX  INSTAL_AMT_PAYMENT_MEAN  \\\n",
       "68219              8855.099609              7472.004395   \n",
       "69323             24729.929688              9144.922852   \n",
       "147830            20831.939453              7979.403809   \n",
       "11904             20717.865234              7712.538574   \n",
       "244391            42710.398438              6453.739258   \n",
       "\n",
       "        INSTAL_AMT_PAYMENT_SUM  INSTAL_DAYS_ENTRY_PAYMENT_MAX  \\\n",
       "68219             1.643841e+05                          -85.0   \n",
       "69323             1.097391e+05                          -23.0   \n",
       "147830            1.994851e+05                          -31.0   \n",
       "11904             1.696758e+06                          -28.0   \n",
       "244391            1.355285e+05                          -11.0   \n",
       "\n",
       "        INSTAL_DAYS_ENTRY_PAYMENT_MEAN  INSTAL_DAYS_ENTRY_PAYMENT_SUM  \\\n",
       "68219                         -634.000                       -13944.0   \n",
       "69323                         -615.000                        -7379.0   \n",
       "147830                        -134.750                        -3368.0   \n",
       "11904                        -1252.000                      -275528.0   \n",
       "244391                        -143.625                        -3017.0   \n",
       "\n",
       "        INSTAL_COUNT  \n",
       "68219           22.0  \n",
       "69323           12.0  \n",
       "147830          25.0  \n",
       "11904          220.0  \n",
       "244391          21.0  \n",
       "\n",
       "[5 rows x 624 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On prend un échantillon de 100 clients \n",
    "df_sub = df.sample(100)\n",
    "#df_sub.drop(columns='TARGET', inplace=True)\n",
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4475a6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.to_csv('echantillon.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "440f5b52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique(df_sub[\"TARGET\"].values.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cd0115",
   "metadata": {},
   "source": [
    "## C) Modélisation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64331694",
   "metadata": {},
   "source": [
    "### 1- Séparation du data set en train et test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a846518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "deleted_cols = ['TARGET','SK_ID_CURR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f08bc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reduce.drop(columns=deleted_cols)\n",
    "y = df_reduce['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "babf1b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, \n",
    "                                                    test_size=0.2,\n",
    "                                                   random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2df1e83c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    226201\n",
       "1     19804\n",
       "Name: TARGET, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8cdc4325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307507, 622)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b513c24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    56481\n",
       "1     5021\n",
       "Name: TARGET, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1acfba7",
   "metadata": {},
   "source": [
    "### 2- Sélection des features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a1e0e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le jeu de données \"HomeCredit_columns_description\"\n",
    "col_description = pd.read_csv(\"C:/Users/salma/OneDrive/Bureau/Projet7/Data/Data_kaggle/HomeCredit_columns_description_encode.csv\", sep=',', encoding=\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0bb5892",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Features [254 259 275 284 363 371 374 400 405 410 420 428 433 448 462 468 472 481\n",
      " 493 499 517 594] are constant.\n",
      "invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "# Create and fit selector\n",
    "selector = SelectKBest(f_classif, k=20)\n",
    "selector.fit(X, y)\n",
    "# Get columns to keep and create new dataframe with those only\n",
    "cols = selector.get_support(indices=True)\n",
    "X_new = X.iloc[:,cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb6e6720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>DAYS_EMPLOYED</th>\n",
       "      <th>REGION_RATING_CLIENT</th>\n",
       "      <th>REGION_RATING_CLIENT_W_CITY</th>\n",
       "      <th>EXT_SOURCE_1</th>\n",
       "      <th>EXT_SOURCE_2</th>\n",
       "      <th>EXT_SOURCE_3</th>\n",
       "      <th>DAYS_EMPLOYED_PERC</th>\n",
       "      <th>NAME_INCOME_TYPE_Working</th>\n",
       "      <th>NAME_EDUCATION_TYPE_Higher education</th>\n",
       "      <th>BURO_DAYS_CREDIT_MIN</th>\n",
       "      <th>BURO_DAYS_CREDIT_MEAN</th>\n",
       "      <th>BURO_DAYS_CREDIT_UPDATE_MEAN</th>\n",
       "      <th>BURO_CREDIT_ACTIVE_Active_MEAN</th>\n",
       "      <th>BURO_CREDIT_ACTIVE_Closed_MEAN</th>\n",
       "      <th>PREV_NAME_CONTRACT_STATUS_Approved_MEAN</th>\n",
       "      <th>PREV_NAME_CONTRACT_STATUS_Refused_MEAN</th>\n",
       "      <th>PREV_CODE_REJECT_REASON_SCOFR_MEAN</th>\n",
       "      <th>PREV_CODE_REJECT_REASON_XAP_MEAN</th>\n",
       "      <th>PREV_NAME_PRODUCT_TYPE_walk-in_MEAN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-9461</td>\n",
       "      <td>-637.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.083008</td>\n",
       "      <td>0.262939</td>\n",
       "      <td>0.139404</td>\n",
       "      <td>0.067322</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1437.0</td>\n",
       "      <td>-874.0</td>\n",
       "      <td>-500.00</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.750</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-16765</td>\n",
       "      <td>-1188.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.311279</td>\n",
       "      <td>0.622070</td>\n",
       "      <td>0.535156</td>\n",
       "      <td>0.070862</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-2586.0</td>\n",
       "      <td>-1401.0</td>\n",
       "      <td>-816.00</td>\n",
       "      <td>0.250</td>\n",
       "      <td>0.750</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-19046</td>\n",
       "      <td>-225.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.505859</td>\n",
       "      <td>0.556152</td>\n",
       "      <td>0.729492</td>\n",
       "      <td>0.011810</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1326.0</td>\n",
       "      <td>-867.0</td>\n",
       "      <td>-532.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-19005</td>\n",
       "      <td>-3040.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.505859</td>\n",
       "      <td>0.650391</td>\n",
       "      <td>0.535156</td>\n",
       "      <td>0.159912</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1827.0</td>\n",
       "      <td>-1051.0</td>\n",
       "      <td>-481.75</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.555664</td>\n",
       "      <td>0.111084</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.888672</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-19932</td>\n",
       "      <td>-3038.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.505859</td>\n",
       "      <td>0.322754</td>\n",
       "      <td>0.535156</td>\n",
       "      <td>0.152466</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1149.0</td>\n",
       "      <td>-1149.0</td>\n",
       "      <td>-783.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DAYS_BIRTH  DAYS_EMPLOYED  REGION_RATING_CLIENT  \\\n",
       "0       -9461         -637.0                     2   \n",
       "1      -16765        -1188.0                     1   \n",
       "2      -19046         -225.0                     2   \n",
       "3      -19005        -3040.0                     2   \n",
       "4      -19932        -3038.0                     2   \n",
       "\n",
       "   REGION_RATING_CLIENT_W_CITY  EXT_SOURCE_1  EXT_SOURCE_2  EXT_SOURCE_3  \\\n",
       "0                            2      0.083008      0.262939      0.139404   \n",
       "1                            1      0.311279      0.622070      0.535156   \n",
       "2                            2      0.505859      0.556152      0.729492   \n",
       "3                            2      0.505859      0.650391      0.535156   \n",
       "4                            2      0.505859      0.322754      0.535156   \n",
       "\n",
       "   DAYS_EMPLOYED_PERC  NAME_INCOME_TYPE_Working  \\\n",
       "0            0.067322                       1.0   \n",
       "1            0.070862                       0.0   \n",
       "2            0.011810                       1.0   \n",
       "3            0.159912                       1.0   \n",
       "4            0.152466                       1.0   \n",
       "\n",
       "   NAME_EDUCATION_TYPE_Higher education  BURO_DAYS_CREDIT_MIN  \\\n",
       "0                                   0.0               -1437.0   \n",
       "1                                   1.0               -2586.0   \n",
       "2                                   0.0               -1326.0   \n",
       "3                                   0.0               -1827.0   \n",
       "4                                   0.0               -1149.0   \n",
       "\n",
       "   BURO_DAYS_CREDIT_MEAN  BURO_DAYS_CREDIT_UPDATE_MEAN  \\\n",
       "0                 -874.0                       -500.00   \n",
       "1                -1401.0                       -816.00   \n",
       "2                 -867.0                       -532.00   \n",
       "3                -1051.0                       -481.75   \n",
       "4                -1149.0                       -783.00   \n",
       "\n",
       "   BURO_CREDIT_ACTIVE_Active_MEAN  BURO_CREDIT_ACTIVE_Closed_MEAN  \\\n",
       "0                           0.250                           0.750   \n",
       "1                           0.250                           0.750   \n",
       "2                           0.000                           1.000   \n",
       "3                           0.375                           0.625   \n",
       "4                           0.000                           1.000   \n",
       "\n",
       "   PREV_NAME_CONTRACT_STATUS_Approved_MEAN  \\\n",
       "0                                 1.000000   \n",
       "1                                 1.000000   \n",
       "2                                 1.000000   \n",
       "3                                 0.555664   \n",
       "4                                 1.000000   \n",
       "\n",
       "   PREV_NAME_CONTRACT_STATUS_Refused_MEAN  PREV_CODE_REJECT_REASON_SCOFR_MEAN  \\\n",
       "0                                0.000000                                 0.0   \n",
       "1                                0.000000                                 0.0   \n",
       "2                                0.000000                                 0.0   \n",
       "3                                0.111084                                 0.0   \n",
       "4                                0.000000                                 0.0   \n",
       "\n",
       "   PREV_CODE_REJECT_REASON_XAP_MEAN  PREV_NAME_PRODUCT_TYPE_walk-in_MEAN  \n",
       "0                          1.000000                             0.000000  \n",
       "1                          1.000000                             0.000000  \n",
       "2                          1.000000                             0.000000  \n",
       "3                          0.888672                             0.000000  \n",
       "4                          1.000000                             0.166626  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f241230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train = X_new.columns.tolist()\n",
    "len(features_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "13610503",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_description = []\n",
    "cols_desc = col_description['Row'].tolist()\n",
    "for feature in features_train : \n",
    "    if feature in cols_desc :\n",
    "        ligne = col_description[col_description['Row'] == feature]['Description'].iloc[0]\n",
    "        l_description.append(ligne)\n",
    "    else : \n",
    "        l_description.append('Pas de description')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be96a320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrer les colonnes et leurs descriptions\n",
    "pickle.dump(features_train, open('features_selected.pkl', 'wb'))\n",
    "pickle.dump(l_description, open('features_description.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced4d0f5",
   "metadata": {},
   "source": [
    "### 3- Modélisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fefe86d",
   "metadata": {},
   "source": [
    "#### (a) Division du dataset en train et test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6ba2d004",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_new,y, \n",
    "                                                    test_size=0.2,\n",
    "                                                   random_state=42,\n",
    "                                                   stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f8ff85",
   "metadata": {},
   "source": [
    "#### (b) Création de la fonction coût métier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe86d944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fct_cout(y_test, y_pred, cout_fp=1, coup_fn=10):\n",
    "    \n",
    "    TN, FP, FN, TP = confusion_matrix(y_test, y_pred).ravel()\n",
    "    resultat = (cout_fp*FP + coup_fn*FN)/(TN + coup_fn*FN + TP + cout_fp*FP)\n",
    "    return resultat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3025b9",
   "metadata": {},
   "source": [
    "On transforme la fct_cout en un score en utilisant la fonction make_scorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b898aa20",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_cout = make_scorer(fct_cout, greater_is_better=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ae2ace",
   "metadata": {},
   "source": [
    "#### (c) Fonctions d'entraînement des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "065f88ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entrainement_model(model, params,X_train,X_test,y_train, y_test):\n",
    "    \n",
    "    \"\"\"Cette fonction renvoie le meilleur modèle et les meilleurs hyperparamètres\n",
    "    obtenues par cross validation avec GridSearchCv\"\"\"\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    \n",
    "    grid_model = GridSearchCV(model, params, cv=5, scoring=score_cout)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    grid_model.fit(X_train,y_train)\n",
    "    tf = time.time()\n",
    "    print(\"Temps d'exécution avec gridSearchCv : {:.4f} seconds\".format(tf - t0))\n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "    best_model = grid_model.best_estimator_\n",
    "    best_params = grid_model.best_params_\n",
    "    \n",
    "\n",
    "    \n",
    "    return best_model, best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2e93bb",
   "metadata": {},
   "source": [
    "#### (d) Fonctions d'évaluation des modèles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "520a1b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_model(model, X_train,X_test,y_train, y_test, smote_method = False):\n",
    "    \n",
    "    \"\"\"Cette fonction entraîne le modèle et affiche les scores du modèle\"\"\"\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    if smote_method:\n",
    "        oversampler=SMOTE(random_state=0)\n",
    "        X_train,y_train=oversampler.fit_resample(X_train,y_train)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    tf = time.time()\n",
    "    print(\"Temps d'exécution du meilleur modèle : {:.4f} seconds\".format(tf - t0))\n",
    "    \n",
    "    \n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    acc_train = accuracy_score(y_train, y_pred_train)\n",
    "    acc_test = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    roc_train = roc_auc_score(y_train, y_pred_train)\n",
    "    roc_test = roc_auc_score(y_test, y_pred_test)\n",
    "    \n",
    "    beta_score_train = fbeta_score(y_train, y_pred_train, beta=10)\n",
    "    beta_score_test = fbeta_score(y_test, y_pred_test, beta=10)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Accuracy train : \", acc_train)\n",
    "    print(\"Accuracy test : \", acc_test)\n",
    "    print(\"Roc Accuracy train : \", roc_train)\n",
    "    print(\"Roc Accuracy test : \", roc_test)\n",
    "    print(\"Beta score train : \", beta_score_train)\n",
    "    print(\"Beta score test : \", beta_score_test)\n",
    "    print(confusion_matrix(y_test, y_pred_test))\n",
    "    print(classification_report(y_test, y_pred_test))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "818c7c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les modèles\n",
    "model_lr = LogisticRegression(random_state=0, class_weight=\"balanced\")\n",
    "model_rf = RandomForestClassifier(random_state=0, class_weight=\"balanced\")\n",
    "model_lgb = LGBMClassifier(random_state=0, class_weight=\"balanced\")\n",
    "model_xgb = XGBClassifier(random_state=0, class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "43085fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_lr = {'C': [0.1, 1, 10, 100]}#,'penalty': ['l1', 'l2']} model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "\n",
    "params_rf = { \n",
    "    'n_estimators': [100, 200],#[int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n",
    "    'max_depth' : [2, 5, 10], #[int(x) for x in np.linspace(10, 110, num = 11)],\n",
    "    'min_samples_split' : [2, 5, 10]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "params_lgb = {'learning_rate': [0.1, 0.01, 0.001],\n",
    "              'max_depth': [3, 4, 5]}\n",
    "\n",
    "params_xgb = {'learning_rate': [0.1, 0.01, 0.001],\n",
    "              'max_depth': [3, 4, 5]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e746dce",
   "metadata": {},
   "source": [
    "#### (a) Pour la régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1013fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution avec gridSearchCv : 41.4059 seconds\n",
      "Temps d'exécution du meilleur modèle : 2.0503 seconds\n",
      "Accuracy train :  0.6828316497632162\n",
      "Accuracy test :  0.6849370752170661\n",
      "Roc Accuracy train :  0.6715778312834204\n",
      "Roc Accuracy test :  0.6769715564356691\n",
      "Beta score train :  0.6376691959211847\n",
      "Beta score test :  0.646740348572091\n",
      "[[38811 17726]\n",
      " [ 1651  3314]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.69      0.80     56537\n",
      "           1       0.16      0.67      0.25      4965\n",
      "\n",
      "    accuracy                           0.68     61502\n",
      "   macro avg       0.56      0.68      0.53     61502\n",
      "weighted avg       0.89      0.68      0.76     61502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_lr, best_param_lr = entrainement_model(model_lr, params_lr,X_train,X_test,y_train, y_test)\n",
    "final_model_lr = LogisticRegression(**best_param_lr, random_state=0, class_weight=\"balanced\")\n",
    "evaluation_model(final_model_lr, X_train,X_test,y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf35054d",
   "metadata": {},
   "source": [
    "#### (b) Pour la forêt aléatoire "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6d70a076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution avec gridSearchCv : 3473.4014 seconds\n",
      "Temps d'exécution du meilleur modèle : 88.1717 seconds\n",
      "Accuracy train :  0.7357695981788988\n",
      "Accuracy test :  0.7256674579688466\n",
      "Roc Accuracy train :  0.717181876865757\n",
      "Roc Accuracy test :  0.6727610349760527\n",
      "Beta score train :  0.6771605849768888\n",
      "Beta score test :  0.594266620793413\n",
      "[[41603 14934]\n",
      " [ 1938  3027]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.74      0.83     56537\n",
      "           1       0.17      0.61      0.26      4965\n",
      "\n",
      "    accuracy                           0.73     61502\n",
      "   macro avg       0.56      0.67      0.55     61502\n",
      "weighted avg       0.89      0.73      0.79     61502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_rf, best_param_rf = entrainement_model(model_rf, params_rf,X_train,X_test,y_train, y_test)\n",
    "final_model_rf = RandomForestClassifier(**best_param_rf, random_state=0, class_weight=\"balanced\")\n",
    "evaluation_model(final_model_rf, X_train,X_test,y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07da91d",
   "metadata": {},
   "source": [
    "#### (c) Le modèle lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5fabad32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temps d'exécution avec gridSearchCv : 118.3620 seconds\n",
      "Temps d'exécution du meilleur modèle : 2.6572 seconds\n",
      "Accuracy train :  0.6920875591959513\n",
      "Accuracy test :  0.6888719066046632\n",
      "Roc Accuracy train :  0.6968446084051136\n",
      "Roc Accuracy test :  0.6823268887127023\n",
      "Beta score train :  0.6808181691686896\n",
      "Beta score test :  0.6537880193595275\n",
      "[[39018 17519]\n",
      " [ 1616  3349]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.69      0.80     56537\n",
      "           1       0.16      0.67      0.26      4965\n",
      "\n",
      "    accuracy                           0.69     61502\n",
      "   macro avg       0.56      0.68      0.53     61502\n",
      "weighted avg       0.90      0.69      0.76     61502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_lgb, best_param_lgb = entrainement_model(model_lgb, params_lgb,X_train,X_test,y_train, y_test)\n",
    "final_model_lgb = LGBMClassifier(**best_param_lgb, random_state=0, class_weight=\"balanced\")\n",
    "evaluation_model(final_model_lgb, X_train,X_test,y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad48f935",
   "metadata": {},
   "source": [
    "#### (d) Le modèle XgBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "250adee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:36:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:36:24] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:36:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:36:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:36:42] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:36:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:36:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:36:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:37:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:37:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:37:10] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:37:11] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:37:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:37:23] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:37:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:37:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:37:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:37:48] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:38:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:38:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:38:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:38:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:38:35] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:38:36] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:38:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:38:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:39:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:39:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:39:29] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:39:29] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:39:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:39:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:39:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:39:55] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:40:04] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:40:04] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:40:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:40:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:40:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:40:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:40:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:40:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:40:43] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:40:44] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:40:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:40:56] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:41:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:41:08] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:41:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:41:20] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:41:32] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:41:33] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:41:49] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:41:49] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:42:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:42:06] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:42:21] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:42:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:42:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:42:38] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:42:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:42:54] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:43:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:43:03] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:43:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:43:12] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:43:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:43:22] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:43:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:43:31] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:43:40] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:43:40] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:43:52] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:43:53] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:44:04] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:44:05] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:44:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:44:17] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:44:29] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:44:29] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:44:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:44:41] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:44:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:44:57] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:45:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:45:13] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:45:29] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:45:29] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:45:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:45:45] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22:46:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:576: \n",
      "Parameters: { \"class_weight\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[22:46:01] WARNING: C:\\Windows\\Temp\\abs_557yfx631l\\croots\\recipe\\xgboost-split_1659548953302\\work\\src\\learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Temps d'exécution avec gridSearchCv : 597.4024 seconds\n",
      "[LightGBM] [Warning] Unknown parameter: use_label_encoder\n",
      "Temps d'exécution du meilleur modèle : 2.7815 seconds\n",
      "Accuracy train :  0.6920875591959513\n",
      "Accuracy test :  0.6888719066046632\n",
      "Roc Accuracy train :  0.6968446084051136\n",
      "Roc Accuracy test :  0.6823268887127023\n",
      "Beta score train :  0.6808181691686896\n",
      "Beta score test :  0.6537880193595275\n",
      "[[39018 17519]\n",
      " [ 1616  3349]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.69      0.80     56537\n",
      "           1       0.16      0.67      0.26      4965\n",
      "\n",
      "    accuracy                           0.69     61502\n",
      "   macro avg       0.56      0.68      0.53     61502\n",
      "weighted avg       0.90      0.69      0.76     61502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_xgb, best_param_xgb = entrainement_model(model_xgb, params_xgb,X_train,X_test,y_train, y_test)\n",
    "final_model_xgb = LGBMClassifier(**best_param_xgb, random_state=0, class_weight=\"balanced\", use_label_encoder=False)\n",
    "evaluation_model(final_model_xgb, X_train,X_test,y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44ea56b",
   "metadata": {},
   "source": [
    "### 4- Modèle final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c42ed83",
   "metadata": {},
   "source": [
    "On choisit comme modèle final le random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a79a9cf",
   "metadata": {},
   "source": [
    "#### (a) Définition du seuil de proba pour la fonction coût personnalisée"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3d95b126",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_seuil(model, X_test, y_test) :\n",
    "    \n",
    "    #y_pred = model.predict(X_test)\n",
    "    \n",
    "    y_scores = model.predict_proba(X_test)[:,1]\n",
    "    best_threshold = 0\n",
    "    best_cost = float('inf')\n",
    "\n",
    "    for threshold in np.linspace(start=0, stop=1, num=100):\n",
    "        y_pred = (y_scores >= threshold).astype(int)\n",
    "        #print(\"y pred : \", y_pred)\n",
    "        total_cost = fct_cout(y_test, y_pred)\n",
    "        if total_cost < best_cost:\n",
    "            best_threshold = threshold\n",
    "            best_cost = total_cost\n",
    "            \n",
    "    y_pred_f = (y_scores >= best_threshold).astype(int)\n",
    "    print(\"prediction : \", y_pred_f)\n",
    "            \n",
    "    return best_threshold\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3bd9097",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_model_final(model, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    \"\"\"Cette fonction donne les scores pour le model final choisi\"\"\"\n",
    "    \n",
    "    \n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    acc_train = accuracy_score(y_train, y_pred_train)\n",
    "    acc_test = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    roc_train = roc_auc_score(y_train, y_pred_train)\n",
    "    roc_test = roc_auc_score(y_test, y_pred_test)\n",
    "    \n",
    "    \n",
    "    beta_score_train = fbeta_score(y_train, y_pred_train, beta=10)\n",
    "    beta_score_test = fbeta_score(y_test, y_pred_test, beta=10)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Accuracy train : \", acc_train)\n",
    "    print(\"Accuracy test : \", acc_test)\n",
    "    print(\"Roc Accuracy train : \", roc_train)\n",
    "    print(\"Roc Accuracy test : \", roc_test)\n",
    "    print(\"Beta score train : \", beta_score_train)\n",
    "    print(\"Beta score test : \", beta_score_test)\n",
    "    print(confusion_matrix(y_test, y_pred_test))\n",
    "    print(classification_report(y_test, y_pred_test))    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9235dc",
   "metadata": {},
   "source": [
    "#### (b) On met le modèle final dans un Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f2255928",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_f = best_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aebb2b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "smt = SMOTE(random_state=42)\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c7481a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "                     ('scaler', scaler),\n",
    "                    ('model', model_f)\n",
    "                          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "15191e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('model',\n",
       "                 RandomForestClassifier(class_weight='balanced', max_depth=10,\n",
       "                                        n_estimators=200, random_state=0))])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a05c3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction :  [0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5858585858585859"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = pred_seuil(pipeline, X_test, y_test)\n",
    "res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a24549eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train :  0.7357695981788988\n",
      "Accuracy test :  0.7256674579688466\n",
      "Roc Accuracy train :  0.717181876865757\n",
      "Roc Accuracy test :  0.6727610349760527\n",
      "Beta score train :  0.6771605849768888\n",
      "Beta score test :  0.594266620793413\n",
      "[[41603 14934]\n",
      " [ 1938  3027]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.74      0.83     56537\n",
      "           1       0.17      0.61      0.26      4965\n",
      "\n",
      "    accuracy                           0.73     61502\n",
      "   macro avg       0.56      0.67      0.55     61502\n",
      "weighted avg       0.89      0.73      0.79     61502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluation_model_final(pipeline,X_train, y_train, X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "76a457de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrer le model logistic regression\n",
    "#pickle.dump(pipeline_lgb, open('model_credit_rl.pkl', 'wb'))\n",
    "pickle.dump(pipeline, open('model_credit_rf.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242ccacd",
   "metadata": {},
   "source": [
    "### 5- Interprétation en utilisant shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "81f63888",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_rf = pickle.load(open(\"model_credit_rf.pkl\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43deb0a3",
   "metadata": {},
   "source": [
    "#### (a) Pour la régression logistique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4bcc0a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pydantic --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad589acb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline_lr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10640\\1584361665.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mexplainer_lr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinearExplainer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline_lr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'model'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexplainer_lr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'shap_explainer_lr.pkl'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pipeline_lr' is not defined"
     ]
    }
   ],
   "source": [
    "explainer_lr = shap.LinearExplainer(pipeline_lr.named_steps['model'], X_train)\n",
    "pickle.dump(explainer_lr, open('shap_explainer_lr.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4db02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_val = explainer_lr(X_test)\n",
    "all_features = list(X_new.columns)\n",
    "shap.summary_plot(shap_val, X_test, feature_names=all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a01dc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(shap_val[0], max_display=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6364250",
   "metadata": {},
   "source": [
    "#### (b) Pour la forêt aléatoire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22448b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = list(X_new.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac72639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer un explicateur SHAP\n",
    "explainer_rf = shap.TreeExplainer(pipeline_rf.named_steps['model'], X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afd0799",
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump(explainer_rf, open('shap_explainer_rf.dill', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a313945",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_fr = dill.load(open(\"shap_explainer_rf.dill\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a718a2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les valeurs SHAP pour les prédictions de test\n",
    "shap_values = explainer_rf.shap_values(X_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58bd16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X_test, feature_names=all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5411aebd",
   "metadata": {},
   "source": [
    "### Pour un client "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ec3996",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sample = df[df[\"SK_ID_CURR\"] == 137021]\n",
    "X_test_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc64bb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test_sample[all_features].columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266e49dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_sample[all_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e40660",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = explainer_fr.shap_values(X_test_sample[all_features])\n",
    "shap_values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdb4c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots._waterfall.waterfall_legacy(explainer_fr.expected_value[0],shap_values[0][0],feature_names = X_test_sample.columns,max_display= 10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0880c09f",
   "metadata": {},
   "source": [
    "## D) MlFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e916b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04503663",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.models.signature import infer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa11378",
   "metadata": {},
   "outputs": [],
   "source": [
    "signature = infer_signature(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1679bea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.sklearn.save_model(pipeline_rf, 'mlflow_model', signature=signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b4c95b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
